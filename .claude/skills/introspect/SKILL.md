---
name: introspect
description: Introspection and self-improvement across hierarchy levels
version: 2.0.0
---

# /introspect - Introspection and Self-Improvement

Unified skill for all introspection levels. L2 performs interactive self-improvement through archive analysis. L3+ synthesizes higher-order insights by analyzing L(N-1) documents to produce L(N) meta-analyses.

---

## Invocation

| Pattern | Behavior |
|---------|----------|
| `/introspect 2` | Full interactive L2 cycle: discovery, selection, dialogue, logging, archive |
| `/introspect 2 --dry-run` | Discovery only, no issue creation |
| `/introspect 3` | Reads all L2 summaries, produces L3 meta-analysis |
| `/introspect 4` | Reads all L3 meta-summaries, produces L4 meta-analysis |
| `/introspect N` | Reads all L(N-1) documents, produces L(N) document (N >= 3) |

**Required argument:** N >= 2. L1 is generated by `/update-and-archive` (not this skill). N < 2 produces an error.

### Input/Output Mapping

| Level | Input Source | Output Path |
|-------|-------------|-------------|
| 2 | `dev/storage/archive/retrospectives/*.md` (L1) + MCP analysis tools | `dev/storage/archive/summaries/<entry-id>.md` |
| 3 | `dev/storage/archive/summaries/*.md` | `dev/storage/archive/meta-summaries/L3-<entry-id>.md` |
| 4+ | `dev/storage/archive/meta-summaries/L<N-1>-*.md` | `dev/storage/archive/meta-summaries/L<N>-<entry-id>.md` |

**Minimum viable input (L3+):** 2+ L(N-1) documents required. Fewer triggers graceful failure.

---

## Mandatory Archive Protocol

**This is not optional. Violations break the skill contract.**

### First Action on Invocation

Before doing ANYTHING else:

1. Call `sbs_archive_state()` via MCP
2. Check `global_state` field:
   - `null` -> Fresh run, proceed
   - `{skill: "introspect", substate: X}` -> Resume from substate X
   - `{skill: "other", ...}` -> Error: state conflict, do NOT proceed

### Phase Transitions

Every phase change MUST use MCP skill lifecycle tools:

**For L2 (`/introspect 2`):**
```
sbs_skill_start(skill="introspect", initial_substate="discovery")
sbs_skill_transition(skill="introspect", to_phase="selection")
sbs_skill_transition(skill="introspect", to_phase="dialogue")
sbs_skill_transition(skill="introspect", to_phase="logging")
sbs_skill_transition(skill="introspect", to_phase="archive")
sbs_skill_end(skill="introspect")
```

Phases: `discovery` -> `selection` -> `dialogue` -> `logging` -> `archive`

**For L3+ (`/introspect N` where N >= 3):**
```
sbs_skill_start(skill="introspect", initial_substate="ingestion")
sbs_skill_transition(skill="introspect", to_phase="synthesis")
sbs_skill_transition(skill="introspect", to_phase="archive")
sbs_skill_end(skill="introspect")
```

Phases: `ingestion` -> `synthesis` -> `archive`

### Ending the Skill

```
sbs_skill_end(skill="introspect")
```

This sets `global_state` to `null`, returning system to idle.

---

## Agent Concurrency

`/introspect 2` supports multiagent execution:
- **Up to 4 `sbs-developer` agents** may run concurrently during discovery and logging phases
- Useful for: parallel pillar analysis (each agent queries different analysis tools), parallel issue creation
- Collision avoidance: agents must target independent analysis dimensions or issue creation

**Phase-specific guidance (L2 only):**

| Phase | Concurrency | Use Case |
|-------|-------------|----------|
| discovery | Up to 4 agents | Each agent analyzes a different pillar |
| selection | 1 agent (user interaction) | Interactive selection requires single thread |
| dialogue | 1 agent (user interaction) | Refinement requires single thread |
| logging | Up to 4 agents | Parallel issue creation via `sbs_issue_log` |
| archive | 1 agent | Final summary is sequential |

**L3+ (`/introspect N` where N >= 3):** No concurrency. Sequential read-synthesize-write operation. No subagent spawning.

---

## Level Dispatch

On invocation, parse N from the argument:

- **N < 2:** Error. Report: "Usage: `/introspect <N>` where N >= 2. Level 1 is generated by `/update-and-archive`."
- **N = 2:** Run L2 phases (discovery -> selection -> dialogue -> logging -> archive). See "L2: Self-Improvement Cycle" below.
- **N >= 3:** Run L(N) phases (ingestion -> synthesis -> archive). See "L3+: Meta-Analysis" below.

---

# L2: Self-Improvement Cycle

Analyze archived Claude sessions to identify patterns and capture actionable improvements.

---

## L2 Phase 1: Discovery

**Purpose:** Query archive, identify patterns, generate findings.

### Entry

```
sbs_skill_start(skill="introspect", initial_substate="discovery")
```

### Actions

**Step 0: Retrospective Review (FIRST)**

Before running any MCP analysis tools, read all session retrospectives generated since the last self-improve cycle. These are L1 introspections (see Introspection Hierarchy below) and are the primary input for discovery.

1. List files in `dev/storage/archive/retrospectives/` that are newer than the last self-improve cycle
   - Use `sbs_entries_since_self_improve` to find the timestamp boundary
   - Read each retrospective file (they are markdown documents with 5 analysis dimensions)
2. Extract observations, patterns, and specific examples from each retrospective
3. Note recurring themes across multiple retrospectives -- these are high-signal findings
4. Identify items that retrospectives flagged but MCP analysis tools would not surface (e.g., user answer patterns, question quality, alignment gaps observed in-the-moment)

The retrospectives contain observations captured while context was hot -- things that automated analysis of archive metadata cannot reconstruct. Treat them as the highest-fidelity input available.

**Step 0.5: Verification Sampling**

Before running automated analysis, verify whether prior improvement guidance has been adopted.

Each L2 cycle selects 2-3 prior guidance additions (from previous improvement cycles) and searches for adoption evidence:

1. **Selection criteria:** Pick 2-3 items from the most recent L2 summary's recommendations or recently created improvement issues. Prefer items that are:
   - At least 1 cycle old (give time for adoption)
   - Concrete enough to search for evidence
   - Not yet verified in a previous cycle

2. **Evidence search:** For each selected item:
   - Search session JSONL files in `dev/storage/archive/sessions/` for behavioral evidence
   - Search archive entries via `sbs_search_entries` for related tags or patterns
   - Check if related issues were closed and if the fix is present in guidance files

3. **Status classification:** Record each item with one of:
   - **ADOPTED** — Clear evidence the guidance is being followed (cite specific session/entry IDs)
   - **NOT YET OBSERVED** — No evidence found, but insufficient data to conclude ineffective
   - **INEFFECTIVE** — Evidence that the guidance is being ignored or is not working as intended

4. **Escalation rule:** If an item has been "NOT YET OBSERVED" for 2 consecutive L2 cycles, escalate to "INEFFECTIVE" and recommend either:
   - Strengthening the guidance (making it more prominent/specific)
   - Removing the guidance (it's not providing value)
   - Converting to an automated check (if the behavior can be validated programmatically)

Carry verification results forward to the Archive phase output (see Verification Sampling section in L2 Phase 5).

**Step 1: Automated Analysis**

5. Query recent archive entries via `sbs_search_entries` or `sbs_epoch_summary`
6. Analyze patterns across the four pillars (see below)
7. Generate list of potential improvements (combining retrospective insights with MCP tool findings)
8. Score findings by impact and actionability

### Per-Pillar Minimum Requirement

Discovery is **not complete** until at least 1 finding exists for each pillar:
- [ ] Pillar 1 (User Effectiveness): Use `sbs_user_patterns()` + `sbs_successful_sessions()`
- [ ] Pillar 2 (Claude Execution): Use `sbs_successful_sessions()` + `sbs_comparative_analysis()`
- [ ] Pillar 3 (Alignment Patterns): Use `sbs_comparative_analysis()`
- [ ] Pillar 4 (System Engineering): Use `sbs_system_health()`

If a pillar genuinely has zero findings after querying all relevant tools, document the absence explicitly:
"Pillar X: No findings. Queried [tool names]. Archive data insufficient for this pillar."

### Output

A structured list of findings, each with:
- **Pillar**: Which pillar it relates to
- **Finding**: What was observed
- **Recommendation**: Proposed action
- **Impact**: Expected benefit (high/medium/low)

---

## L2 Phase 2: Selection

**Purpose:** Present summary, user picks items for refinement.

### Entry

```
sbs_skill_transition(skill="introspect", to_phase="selection")
```

### Actions

1. Present findings in GUI-style format with checkbox options:
   ```
   Select findings to refine (comma-separated numbers, or 'all'):

   [ ] 1. [User] Alignment questions could be more structured
   [ ] 2. [Claude] Tool call batching suboptimal in graph work
   [ ] 3. [Alignment] Recovery semantics unclear after compaction
   [ ] 4. [System] Build caching not leveraged for incremental work
   ```

2. Wait for user selection

### Output

List of selected finding indices for dialogue phase.

---

## L2 Phase 3: Dialogue

**Purpose:** Refine each selected finding via discussion.

### Entry

```
sbs_skill_transition(skill="introspect", to_phase="dialogue")
```

### Actions

For each selected finding:
1. Present the finding with context
2. Ask clarifying questions:
   - Is this accurately characterized?
   - What's the root cause?
   - What would a good solution look like?
3. Refine into actionable issue specification
4. Confirm with user before proceeding

### Output

Refined issue specifications ready for logging.

---

## L2 Phase 4: Logging

**Purpose:** Log confirmed items as GitHub issues.

### Entry

```
sbs_skill_transition(skill="introspect", to_phase="logging")
```

### Actions

For each confirmed improvement:
1. Infer labels from the finding (see Finding-to-Label Mapping below)
2. Create issue via `sbs_issue_log`:
   - Title: Clear, actionable description
   - Body: Context from dialogue, recommended approach
   - Labels: `["origin:self-improve", ...inferred labels]` (`origin:agent` and `ai-authored` are auto-added by the tool)
3. Track created issue numbers

Example call:
```python
sbs_issue_log(
    title="Add tool call batching guidance to sbs-developer agent",
    body="...",
    labels=[
        "origin:self-improve",
        "feature:enhancement",
        "area:devtools:skills",
        "pillar:claude-execution",
        "impact:performance",
        "friction:slow-feedback",
    ]
)
```

### Output

List of created issue numbers with URLs.

### Finding-to-Label Mapping

Every issue created by `/introspect 2` includes:

**Always present:**
- `origin:self-improve`
- `ai-authored`

**Type label** -- inferred from finding content:

| Signal | Label |
|--------|-------|
| severity: high + category contains "error"/"failure"/"broken" | `bug:functional` |
| category contains "build"/"compile"/"lake" | `bug:build` |
| category contains "data"/"ledger"/"manifest" | `bug:data` |
| category contains "visual"/"render"/"css" | `bug:visual` |
| recommendation contains "add"/"implement"/"create" (new capability) | `feature:new` |
| recommendation contains "improve"/"enhance"/"optimize" | `feature:enhancement` |
| recommendation contains "connect"/"integrate"/"bridge" | `feature:integration` |
| recommendation contains "investigate"/"debug"/"profile" | `investigation` |
| recommendation contains "document"/"clarify" | `housekeeping:docs` |
| recommendation contains "clean"/"remove"/"simplify" | `housekeeping:cleanup` |
| Default | `housekeeping:tooling` |

**Area label** -- inferred from finding category and evidence (entry IDs link to repos):
- Map to the most specific `area:sbs:*`, `area:devtools:*`, or `area:lean:*` label
- When uncertain, prefer the broader area (e.g., `area:devtools:cli` over `area:devtools:gates`)

**Pillar label** -- direct mapping from `finding.pillar`:

| Pillar | Label |
|--------|-------|
| User Effectiveness | `pillar:user-effectiveness` |
| Claude Execution | `pillar:claude-execution` |
| Alignment Patterns | `pillar:alignment-patterns` |
| System Engineering | `pillar:system-engineering` |

**Impact labels** (optional, multi-select) -- inferred from finding content:
- Performance-related findings -> `impact:performance`
- Visual/UI findings -> `impact:visual`
- Developer workflow findings -> `impact:dx`
- Data quality findings -> `impact:data-quality`
- Alignment-related findings -> `impact:alignment`

**Friction labels** (optional) -- inferred from finding description keywords:

| Keyword Pattern | Label |
|-----------------|-------|
| "compaction"/"context lost"/"state recovery" | `friction:context-loss` |
| "misunderstanding"/"different understanding" | `friction:alignment-gap` |
| "missing tool"/"no way to"/"needed capability" | `friction:tooling-gap` |
| "slow"/"wait"/"rebuild"/"iteration time" | `friction:slow-feedback` |
| "manual"/"by hand"/"should be automated" | `friction:manual-step` |
| "submodule"/"cross-repo"/"dependency chain" | `friction:cross-repo` |
| "state confusion"/"orphaned"/"stale state" | `friction:state-confusion` |
| "noise"/"false positive"/"buried" | `friction:signal-noise` |
| "repeated"/"duplicate"/"same work again" | `friction:repeated-work` |
| "no data"/"not captured"/"missing metrics" | `friction:missing-data` |
| "too many"/"cognitive load"/"overwhelm" | `friction:cognitive-load` |

---

## L2 Phase 5: Archive

**Purpose:** Record cycle completion with summary.

### Entry

```
sbs_skill_transition(skill="introspect", to_phase="archive")
```

### Actions

1. Write self-improvement summary document (L2 introspection):
   - **Path:** `dev/storage/archive/summaries/<entry-id>.md`
   - **Content:** A structured markdown document synthesizing observations across all retrospectives (L1) read during discovery, combined with MCP analysis findings. Must include:
     - **Retrospectives reviewed:** Count and entry IDs
     - **Cross-session patterns:** Themes that appeared in multiple retrospectives
     - **Per-pillar synthesis:** What the combined L1 + automated analysis revealed for each pillar
     - **Behavioral observations:** Longer-term patterns visible only when viewing multiple sessions together (e.g., how user communication style evolved, recurring alignment gaps, tool usage trends)
     - **Verification sampling results:** Table from Step 0.5 (see template below)
     - **Findings logged:** Issue numbers and titles
     - **Recommendations for next cycle:** What the next self-improve should pay attention to
   - This document is the L2 introspection -- it observes patterns across L1 documents that no single retrospective could see
   - The **Verification Sampling** section uses this template:

     ```markdown
     ## Verification Sampling

     | Guidance Item | Source | Status | Evidence |
     |---------------|--------|--------|----------|
     | <description> | L2-<id> / Issue #N | ADOPTED / NOT YET OBSERVED / INEFFECTIVE | <cite session/entry IDs or "no evidence found"> |

     **Escalations:** <items hitting 2-cycle threshold, or "none">
     ```
2. Generate cycle summary:
   - Findings discovered: N
   - Findings selected: M
   - Issues created: K
   - Issue numbers: [#X, #Y, #Z]
   - Summary document path
3. Archive with summary data
4. Clear global state:
   ```
   sbs_skill_end(skill="introspect")
   ```

---

## Four Pillars Framework

All L2 analysis is organized across four pillars:

### 1. User Effectiveness

*How well does the system serve the user?*

| Signal | Indicates |
|--------|-----------|
| Multiple entries in same substate | Retry pattern, unclear requirements |
| Long dialogue chains in alignment | Questions too broad |
| `notes` field with corrections | Misunderstanding that needed fixing |
| Repeated scope clarifications | Initial alignment insufficient |

**Good patterns to preserve:**
- Structured options in questions
- Explicit confirmation before major actions
- Clear summary after each phase

### 2. Claude Execution

*How efficiently does Claude perform tasks?*

| Signal | Indicates |
|--------|-----------|
| High entry count in execution | Excessive iterations |
| `gate_validation` failures | Quality issues not caught early |
| Repeated builds without changes | Cache not leveraged |
| Multiple attempts at same fix | Incorrect diagnosis |

**Good patterns to preserve:**
- Batched tool calls
- Incremental validation
- Pattern reuse from similar tasks

### 3. Alignment Patterns

*How well do user and Claude stay aligned?*

| Signal | Indicates |
|--------|-----------|
| Mismatched `global_state` | State tracking failure |
| Plan file changes mid-execution | Scope creep or poor initial planning |
| Skip from alignment to execution | Planning phase bypassed |
| Multiple skill invocations for one task | Fragmented work |

**Good patterns to preserve:**
- Explicit phase transitions
- Plan approval before execution
- State verification after compaction

### 4. System Engineering

*How well does the tooling support the work?*

| Signal | Indicates |
|--------|-----------|
| Long build times in `quality_scores` | Performance bottleneck |
| Build retries | Flaky infrastructure |
| Validator false positives | Overly strict rules |
| Missing error context | Poor error messages |

**Good patterns to preserve:**
- Incremental builds
- Early validation
- Actionable error messages

---

## L2 Analysis Workflow

### Step 1: Gather Data

```python
# Get current epoch summary
epoch = sbs_epoch_summary()

# Search for recent entries
entries = sbs_search_entries(limit=50, trigger="skill")

# Get state context
context = sbs_context(include=["state", "epoch", "recent"])
```

### Step 2: Pattern Detection

For each pillar, scan entries for signals:

1. **Count patterns**: How often does each signal appear?
2. **Cluster by cause**: Group related signals
3. **Score by impact**: Which patterns hurt most?

### Step 3: Generate Findings

Each finding should have:
- **Pillar**: Which area it affects
- **Evidence**: Specific entry IDs showing the pattern
- **Frequency**: How often this occurs
- **Impact**: Estimated cost (time, quality, frustration)
- **Recommendation**: Concrete action to improve

### Step 4: Prioritize

Sort findings by:
1. Impact (high/medium/low)
2. Actionability (can we fix this now?)
3. Scope (narrow fix vs architectural change)

---

## L2 Finding Template

```yaml
finding:
  pillar: "Claude Execution"
  title: "Tool calls not batched in graph debugging"
  evidence:
    - entry_id: "1700000045"
      observation: "3 sequential Read calls that could be parallel"
    - entry_id: "1700000046"
      observation: "Same pattern repeated"
  frequency: "5 occurrences in 12 entries"
  impact: "high - adds ~30s per iteration"
  recommendation: |
    When reading multiple independent files, batch Read
    calls in a single message. The agent system prompt
    should emphasize parallel tool calls for independent
    operations.
  issue_type: "feature"
  labels:
    - "origin:self-improve"
    - "ai-authored"
    - "feature:enhancement"
    - "area:devtools:mcp"
    - "pillar:claude-execution"
    - "impact:performance"
    - "friction:slow-feedback"
```

---

## L2 Output Formats

### Discovery Phase Output

```markdown
## Findings Summary

### High Impact (3)
1. [Claude] Tool call batching suboptimal
2. [System] CSS rebuilds not cached
3. [User] Alignment questions too broad

### Medium Impact (2)
4. [Alignment] Recovery after compaction unclear
5. [Claude] Error recovery inconsistent

### Low Impact (1)
6. [System] Verbose logging in builds
```

### Dialogue Phase Output

```markdown
## Refined Finding: Tool Call Batching

**Original observation:**
3 sequential Read calls that could be parallel in graph debugging.

**Root cause:**
Default behavior is sequential tool calls. Agent system prompt
doesn't emphasize batching for independent operations.

**Proposed solution:**
Add explicit guidance to sbs-developer.md about batching
independent tool calls.

**Success criteria:**
Next graph debugging task shows batched Read calls in archive.

**Issue specification:**
- Title: Add tool call batching guidance to sbs-developer agent
- Type: feature
- Body: [full context above]
```

---

## L2 Dry Run Mode

When invoked with `/introspect 2 --dry-run`:
- Execute discovery and selection phases normally
- Skip dialogue, logging, and archive phases
- Report what would have been created
- Do NOT modify any state

---

## L2 Example Session

```
User: /introspect 2

Claude: [Checking archive state...]
        Global state is null. Starting fresh discovery.

[Phase 1: Discovery]
Analyzing 47 entries from the current epoch...

Found 6 potential improvements:

Claude: [Phase 2: Selection]
Select findings to refine:

[1] [User] Alignment questions often too broad, causing back-and-forth
[2] [Claude] Graph layout debugging took 3x expected iterations
[3] [Alignment] Plan was modified 4 times during single task
[4] [System] Same CSS file rebuilt 12 times without changes

Enter numbers (comma-separated) or 'all': 1, 4

User: 1, 4

Claude: [Phase 3: Dialogue]

Finding 1: Alignment questions too broad
- Observed in entries: 1700000045, 1700000046, 1700000047
- Pattern: "What would you like to do?" followed by 3+ clarifications

Is this accurately characterized? Would structured options help?

User: Yes, I'd prefer numbered options with clear scope

Claude: Refined recommendation:
  Title: Add structured options to alignment phase questions
  Body: When asking for task clarification, provide 3-5 specific
        options with defined scope rather than open-ended questions.
  Type: feature

Proceed with this specification? [Y/n]

User: y

[... continues through all selected findings ...]

Claude: [Phase 4: Logging]
Created issues:
- #47: Add structured options to alignment phase questions
- #48: Cache CSS compilation during incremental builds

[Phase 5: Archive]
Cycle complete:
- Findings discovered: 6
- Findings refined: 2
- Issues created: 2

State cleared. Returning to idle.
```

---

## L2 Labels

Issues created by `/introspect 2` use the enriched label taxonomy defined in
`dev/storage/taxonomy.yaml`. Every issue includes `origin:self-improve`
and `ai-authored`, plus labels from the type, area, pillar, impact, and friction
dimensions as inferred from the finding content (see Finding-to-Label Mapping
in L2 Phase 4 above).

The old `label`/`area` parameters on `sbs_issue_create` are superseded by
the `labels` list parameter, which accepts any label name from the taxonomy.

---

# L3+: Meta-Analysis

Synthesize higher-order insights by analyzing L(N-1) introspection documents to produce L(N) meta-analyses. Each level adds observational power that the level below cannot achieve individually.

---

## L3+ Phase 1: Ingestion

**Purpose:** Collect and parse all L(N-1) documents.

### Entry

```
sbs_skill_start(skill="introspect", initial_substate="ingestion")
```

### Actions

1. Parse N from invocation argument. Reject if N < 3 (L2 uses a different phase sequence).
2. Glob for L(N-1) documents:
   - **N=3:** `dev/storage/archive/summaries/*.md` (all L2 summaries, excluding `.gitkeep`)
   - **N=4+:** `dev/storage/archive/meta-summaries/L<N-1>-*.md`
3. Count documents. If < 2, fail gracefully (see Error Handling).
4. Read every document.
5. Extract structural elements from each:
   - Dates and cycle identifiers
   - Findings logged (issue numbers, titles)
   - Cross-session patterns identified
   - Per-pillar synthesis content
   - Behavioral observations
   - Recommendations made
   - Metric values (quality scores, completion rates, etc.)
6. Build a chronological timeline of the L(N-1) series.

### Output

Structured inventory of all L(N-1) content, organized chronologically.

---

## L3+ Phase 2: Synthesis

**Purpose:** Answer questions that L(N-1) documents cannot answer individually.

### Entry

```
sbs_skill_transition(skill="introspect", to_phase="synthesis")
```

### Required Analysis Dimensions

1. **Skill Evolution Trajectory**
   How has the improvement system itself changed across cycles? Track: finding categories, finding depth, closure rates, verification gaps. Identify whether the system is getting better at finding problems, or just finding different ones.

2. **Recurring Friction Inventory**
   Problems appearing in 3+ L(N-1) documents despite being "addressed." What persistence reveals about intervention strategies. Distinguish between: genuinely hard problems, insufficiently scoped fixes, and fixes that were never verified.

3. **Metric Trajectory Analysis**
   Track quantitative metrics across all L(N-1) documents. Identify plateaus, improvements, regressions, and stuck areas. Present as tables with directional indicators.

4. **Intervention Effectiveness**
   Classify past interventions by type (guidance addition, code fix, architectural redirect, investigation). Compute resolution rate per type. Identify which intervention types produce verifiable results vs. which produce only stated intentions.

5. **Observation Layer Meta-Assessment**
   Is the improvement system observing the right things? Assess: coverage gaps (what isn't being measured), false confidence (metrics that look good but miss reality), signal-to-noise (are findings getting more actionable over time).

6. **Data Speaks**
   Raw aggregated tables from all L(N-1) documents without commentary. Issue counts, pillar distributions, finding categories, metric values. Let the data surface patterns that narrative analysis might miss.

7. **Recommendations for Next Cycle**
   Concrete focus areas for the next L(N) analysis. What data should be collected, what questions remain open, what hypotheses deserve testing.

### Output Structure

```markdown
# Meta-Improvement Analysis: L<N>-<entry-id>

**Level:** <N>
**Date:** <date>
**L(N-1) Documents Analyzed:** <count> (<list of IDs>)
**Date Range:** <first date> to <last date>

---

## 1. Skill Evolution Trajectory
## 2. Recurring Friction Inventory
## 3. Metric Trajectory Analysis
## 4. Intervention Effectiveness
## 5. Observation Layer Meta-Assessment
## 6. Data Speaks
## 7. Recommendations for Next L<N> Cycle
```

---

## L3+ Phase 3: Archive

**Purpose:** Persist the L(N) document and clear state.

### Entry

```
sbs_skill_transition(skill="introspect", to_phase="archive")
```

### Actions

1. Write L(N) document to `dev/storage/archive/meta-summaries/L<N>-<entry-id>.md`
2. Report summary to user:
   - Level produced
   - L(N-1) documents analyzed (count and IDs)
   - Date range covered
   - Key findings (2-3 sentence summary)
   - Output file path
3. Clear global state:
   ```
   sbs_skill_end(skill="introspect")
   ```

---

## Introspection Hierarchy

Self-improvement operates on a hierarchy of introspection levels:

| Level | Name | Generated By | Stored At | Observes |
|-------|------|-------------|-----------|----------|
| L1 | Session Retrospective | `/update-and-archive` (Part -1) | `dev/storage/archive/retrospectives/<id>.md` | Single session: user patterns, alignment gaps, plan execution |
| L2 | Self-Improvement Summary | `/introspect 2` (Phase 5: Archive) | `dev/storage/archive/summaries/<id>.md` | Multiple L1 docs: cross-session patterns, behavioral trends |
| L3 | Meta-Improvement Analysis | `/introspect 3` | `dev/storage/archive/meta-summaries/L3-<id>.md` | Multiple L2 docs: skill evolution, recurring friction, intervention effectiveness |
| L(N) | Higher-Order Analysis | `/introspect <N>` (N >= 4) | `dev/storage/archive/meta-summaries/L<N>-<id>.md` | Multiple L(N-1) docs: structural insights across improvement cycles |

**Key design principle:** Each level reads the outputs of the level below. L2 reads all L1 documents since the last self-improve cycle. L3 reads all L2 documents since the last L3 cycle via `/introspect 3`.

**Why this matters:** Single-session retrospectives (L1) capture observations while context is hot but cannot see cross-session patterns. The self-improvement summary (L2) synthesizes across sessions and can identify trends invisible to any single retrospective -- e.g., "the user has answered the same type of question 5 times across 3 sessions, suggesting a documentation gap."

As the project grows, higher levels emerge naturally. The hierarchical structure ensures each level adds genuine observational power rather than just repeating lower-level findings.

---

## Tool Inventory

### Archive Tools

| Tool | Use For |
|------|---------|
| `sbs_archive_state` | Check current global state |
| `sbs_skill_start` | Claim global state |
| `sbs_skill_transition` | Move between phases |
| `sbs_skill_end` | Release global state |
| `sbs_skill_fail` | Record failure and release state |
| `sbs_search_entries` | Query entries by tag, project, or trigger |
| `sbs_epoch_summary` | Get aggregated epoch statistics |
| `sbs_context` | Build context blocks for analysis |

### Analysis Tools (L2)

| Tool | Use For | Pillars |
|------|---------|---------|
| `sbs_analysis_summary` | Overall archive statistics and basic findings | All |
| `sbs_entries_since_self_improve` | Entries since last self-improve cycle | All |
| `sbs_successful_sessions` | Mine completed tasks, clean execution, high quality | 1, 2 |
| `sbs_comparative_analysis` | Approved vs rejected plans, discriminating features | 2, 3 |
| `sbs_system_health` | Build metrics, quality coverage, tag noise | 4 |
| `sbs_user_patterns` | Alignment efficiency, issue-driven patterns | 1 |
| `sbs_skill_stats` | Per-skill lifecycle metrics (count, rate, duration, failures) | 1, 2 |
| `sbs_phase_transition_health` | Phase transition patterns and anomalies | 2, 3 |
| `sbs_interruption_analysis` | User correction/redirection detection | 1, 3 |
| `sbs_gate_failures` | Gate validation failure analysis | 4 |
| `sbs_tag_effectiveness` | Auto-tag signal-to-noise ratio | 4 |

### Search Tools

| Tool | Use For |
|------|---------|
| `Glob` | Find L(N-1) documents by pattern |
| `Read` | Read each L(N-1) document or archive entry |
| `Grep` | Search for patterns across documents or skill/agent files |

### Issue Tools

| Tool | Use For |
|------|---------|
| `sbs_issue_log` | Create improvement issues (preferred -- auto-populates context) |
| `sbs_issue_create` | Create issues when manual label control needed |
| `sbs_issue_list` | Check for duplicate issues |

---

## Recovery Semantics

### Compaction Survival

If context compacts mid-skill:

1. New context queries `sbs_archive_state()`
2. Reads current `global_state.substate`
3. Resumes from **start** of that substate

### Substate Resume Behavior (L2)

| Substate | Resume Action |
|----------|---------------|
| `discovery` | Re-read retrospectives, re-query archive, regenerate findings |
| `selection` | Show findings again, re-prompt for selection |
| `dialogue` | Check which findings were already refined, continue with remaining |
| `logging` | Check which issues were created, create remaining |
| `archive` | Re-generate summary document (L2), complete archival |

### Substate Resume Behavior (L3+)

| Substate | Resume Action |
|----------|---------------|
| `ingestion` | Re-glob and re-read all L(N-1) documents from scratch |
| `synthesis` | Check if L(N) document already written. If yes, skip to archive. If no, re-synthesize. |
| `archive` | Re-read L(N) document, complete archival and state cleanup |

### State Conflict

If `global_state.skill != "introspect"`:
- Another skill owns the state
- Do NOT proceed
- Report error: "State conflict: skill '<other_skill>' currently owns global state"
- Wait for user resolution

---

## Error Handling

| Error | Response |
|-------|----------|
| N < 2 | Report: "Usage: `/introspect <N>` where N >= 2. Level 1 is generated by `/update-and-archive`." |
| N = 2, no archive entries found | Report "No entries in current epoch" and exit cleanly |
| N = 2, MCP tool fails | Report error, do not proceed, keep state for retry |
| N = 2, user cancels mid-dialogue | Archive partial progress, clear state |
| N = 2, issue creation fails | Report failure, offer retry or skip |
| N >= 3, no L(N-1) documents found | `sbs_skill_fail(skill="introspect", reason="No L(N-1) documents found for level <N>")` |
| N >= 3, only 1 L(N-1) document | `sbs_skill_fail(skill="introspect", reason="Insufficient data: need >= 2 L(N-1) documents, found 1")` |
| State conflict | Report and wait for user resolution (do not force) |
| Glob returns only `.gitkeep` | Treat as zero documents -- fail gracefully |

---

## Anti-Patterns

### Analysis Anti-Patterns

- **Vague findings**: "Things could be better" is not actionable
- **Missing evidence**: Every finding needs specific entry IDs
- **Scope creep**: Stick to what's observable in the archive
- **Blame assignment**: Focus on systems, not individuals
- **Parroting L(N-1) content** (L3+): Restating findings from lower levels adds no value. Each level must produce insights that require seeing multiple L(N-1) documents together.
- **Vague meta-observations** (L3+): "The system is improving" without specific evidence is noise. Cite document IDs and metrics.
- **Ignoring the Data Speaks section** (L3+): Narrative analysis is subjective. Raw tables ground the analysis in fact.
- **Scope creep into implementation** (L3+): `/introspect` analyzes and recommends. It does not create issues (except at L2), modify code, or change guidance. Those are `/task` responsibilities.

### Recommendation Anti-Patterns

- **Unactionable**: "Be more careful" is not a recommendation
- **Architectural rewrites**: Prefer incremental improvements
- **Missing context**: Include what the current behavior is
- **No success criteria**: How do we know if the fix worked?

### Structural Anti-Patterns

- **Skipping ingestion** (L3+): Reading 2 documents and declaring it sufficient when 5 exist. Read ALL L(N-1) documents.
- **Producing L(N) without minimum input** (L3+): The 2-document minimum exists because meta-analysis of a single document is just a restatement. Fail gracefully instead.
- **Mixing levels** (L3+): An L3 document must analyze L2 documents only, not reach back to L1 or raw archive entries. Each level trusts its inputs.
