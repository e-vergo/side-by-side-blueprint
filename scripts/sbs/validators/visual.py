"""
Visual compliance validator using AI vision analysis.

This validator wraps the existing validation logic from validate.py into
the pluggable validator system. It delegates to the AI vision analysis
functions rather than duplicating them.
"""

from __future__ import annotations

from pathlib import Path
from typing import Any, Optional

from .base import BaseValidator, ValidationContext, ValidatorResult
from .registry import register_validator

from ..criteria import (
    PAGE_CRITERIA,
    GLOBAL_CRITERIA,
    SIDEBAR_CRITERIA,
    get_criteria_for_page,
    get_all_criteria_ids,
    format_criteria_for_prompt,
)
from ..validate import (
    generate_page_validation_prompt,
    generate_interaction_validation_prompt,
    generate_final_review_prompt,
    parse_validation_response,
    get_screenshot_path,
    get_all_screenshot_paths,
    ValidationResult,
)
from ..mapping import ALL_PAGES


@register_validator
class VisualValidator(BaseValidator):
    """Visual compliance validator using AI vision analysis.

    This validator checks screenshots against visual compliance criteria.
    It generates prompts for AI vision analysis and parses the results.

    The actual vision analysis is performed by Claude agents using the
    prompts generated by this validator. This design allows the validation
    logic to remain centralized while fitting into the plugin system.

    Usage:
        context = ValidationContext(
            project="SBSTest",
            project_root=Path("/path/to/project"),
            commit="abc123",
            screenshots_dir=Path("/path/to/images/SBSTest/latest"),
            extra={"pages": ["dashboard", "dep_graph"]}  # Optional filter
        )
        result = validator.validate(context)

    Context.extra keys:
        pages: Optional[list[str]] - Specific pages to validate (default: all)
        include_interactive: bool - Include interactive state validation (default: True)
    """

    def __init__(self) -> None:
        super().__init__("visual-compliance", "visual")

    def validate(self, context: ValidationContext) -> ValidatorResult:
        """Execute visual validation against screenshots.

        Generates validation prompts for each page and returns a result
        indicating which pages need validation. The actual AI vision
        analysis is performed externally using the generated prompts.

        Args:
            context: Validation context with project info and paths.

        Returns:
            ValidatorResult with validation prompts and criteria info.
            The 'details' field contains prompts for AI vision analysis.
        """
        # Determine which pages to validate
        pages_filter = context.extra.get("pages")
        if pages_filter:
            pages_to_validate = [p for p in pages_filter if p in PAGE_CRITERIA]
        else:
            pages_to_validate = list(ALL_PAGES)

        # Check screenshots directory exists
        screenshots_dir = context.screenshots_dir
        if not screenshots_dir or not screenshots_dir.exists():
            return self._make_fail(
                findings=["Screenshots directory not found or not specified"],
                details={"screenshots_dir": str(screenshots_dir) if screenshots_dir else None},
            )

        # Build validation prompts for each page
        prompts: dict[str, str] = {}
        criteria_by_page: dict[str, list[str]] = {}
        missing_screenshots: list[str] = []

        for page in pages_to_validate:
            screenshot_path = get_screenshot_path(context.project, page)

            if not screenshot_path.exists():
                missing_screenshots.append(page)
                continue

            # Generate prompt for this page
            prompt = generate_page_validation_prompt(
                page=page,
                screenshot_path=screenshot_path,
                project=context.project,
            )
            prompts[page] = prompt

            # Track criteria for this page
            criteria_by_page[page] = get_all_criteria_ids(page)

        # Handle interactive states if requested
        include_interactive = context.extra.get("include_interactive", True)
        interactive_prompts: dict[str, str] = {}

        if include_interactive:
            for page in pages_to_validate:
                page_criteria = PAGE_CRITERIA.get(page)
                if not page_criteria:
                    continue

                for element in page_criteria.interactive_elements:
                    interaction_id = element.get("id", "unknown")
                    interaction_screenshot = get_screenshot_path(
                        context.project, page, interaction_id
                    )

                    if interaction_screenshot.exists():
                        baseline = get_screenshot_path(context.project, page)
                        prompt = generate_interaction_validation_prompt(
                            page=page,
                            interaction=interaction_id,
                            screenshot_path=interaction_screenshot,
                            baseline_path=baseline if baseline.exists() else None,
                            project=context.project,
                        )
                        interactive_prompts[f"{page}_{interaction_id}"] = prompt

        # Build result
        findings = []
        if missing_screenshots:
            findings.append(
                f"Missing screenshots for: {', '.join(missing_screenshots)}"
            )

        # Calculate totals
        total_pages = len(pages_to_validate)
        pages_with_screenshots = len(prompts)
        total_criteria = sum(len(criteria_by_page.get(p, [])) for p in prompts.keys())

        # This validator returns "passed=True" if we successfully generated prompts
        # The actual pass/fail determination happens when AI analyzes the screenshots
        passed = pages_with_screenshots > 0

        return ValidatorResult(
            validator=self.name,
            passed=passed,
            findings=findings,
            confidence=1.0 if passed else 0.0,
            criteria_results={},  # Populated after AI analysis
            metrics={
                "total_pages": total_pages,
                "pages_with_screenshots": pages_with_screenshots,
                "missing_screenshots": len(missing_screenshots),
                "total_criteria": total_criteria,
                "interactive_states": len(interactive_prompts),
            },
            details={
                "prompts": prompts,
                "interactive_prompts": interactive_prompts,
                "criteria_by_page": criteria_by_page,
                "missing_screenshots": missing_screenshots,
            },
        )

    def parse_ai_response(
        self,
        response: str,
        page: str,
        interaction: Optional[str] = None,
    ) -> ValidationResult:
        """Parse an AI agent's validation response.

        Delegates to the existing parse_validation_response function.

        Args:
            response: Raw response text from AI vision analysis.
            page: Page that was validated.
            interaction: Optional interaction state that was validated.

        Returns:
            ValidationResult with parsed pass/fail and findings.
        """
        return parse_validation_response(response, page, interaction)

    def aggregate_results(
        self,
        results: list[ValidationResult],
    ) -> ValidatorResult:
        """Aggregate multiple validation results into a single ValidatorResult.

        Combines results from multiple pages/interactions into a single
        overall result for reporting.

        Args:
            results: List of ValidationResult from AI analysis.

        Returns:
            ValidatorResult with aggregated pass/fail and criteria results.
        """
        if not results:
            return self._make_fail(findings=["No validation results to aggregate"])

        # Aggregate findings
        all_findings: list[str] = []
        criteria_results: dict[str, bool] = {}
        total_confidence = 0.0
        pages_passed = 0
        pages_failed = 0

        for result in results:
            if result.passed:
                pages_passed += 1
            else:
                pages_failed += 1
                # Add findings with page context
                for finding in result.findings:
                    page_context = result.page
                    if result.interaction:
                        page_context = f"{result.page}/{result.interaction}"
                    all_findings.append(f"[{page_context}] {finding}")

            total_confidence += result.confidence

            # Aggregate criteria results
            for criterion_id in result.criteria_checked:
                # Mark as passed only if ALL pages pass this criterion
                if criterion_id not in criteria_results:
                    criteria_results[criterion_id] = result.passed
                elif not result.passed:
                    criteria_results[criterion_id] = False

        # Overall pass if all pages passed
        overall_passed = pages_failed == 0 and pages_passed > 0
        avg_confidence = total_confidence / len(results) if results else 0.0

        return ValidatorResult(
            validator=self.name,
            passed=overall_passed,
            findings=all_findings,
            confidence=avg_confidence,
            criteria_results=criteria_results,
            metrics={
                "pages_passed": pages_passed,
                "pages_failed": pages_failed,
                "total_results": len(results),
            },
        )

    def get_final_review_prompt(
        self,
        context: ValidationContext,
    ) -> Optional[str]:
        """Generate a prompt for final comprehensive review.

        This prompt is used after all individual pages pass to perform
        a final cross-page consistency check.

        Args:
            context: Validation context.

        Returns:
            Prompt string for final review, or None if no screenshots.
        """
        screenshot_paths = get_all_screenshot_paths(context.project)
        if not screenshot_paths:
            return None

        return generate_final_review_prompt(
            screenshot_paths=screenshot_paths,
            project=context.project,
        )

    @staticmethod
    def get_page_criteria(page: str) -> tuple[list[Any], list[Any]]:
        """Get criteria for a specific page.

        Args:
            page: Page name.

        Returns:
            Tuple of (page_specific_criteria, global_criteria).
        """
        return get_criteria_for_page(page)

    @staticmethod
    def get_all_pages() -> list[str]:
        """Get list of all known page types.

        Returns:
            List of page names that can be validated.
        """
        return list(PAGE_CRITERIA.keys())

    @staticmethod
    def format_criteria(page: str) -> str:
        """Format criteria for a page as human-readable text.

        Args:
            page: Page name.

        Returns:
            Formatted string of criteria for prompt inclusion.
        """
        return format_criteria_for_prompt(page)
